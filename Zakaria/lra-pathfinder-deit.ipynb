{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10066661,"sourceType":"datasetVersion","datasetId":6204149}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torchvision import transforms, models\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport pytorch_lightning as pl\nimport torch\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm import tqdm\nfrom timm import create_model  # For DeiT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:45:57.691870Z","iopub.execute_input":"2024-12-04T15:45:57.695756Z","iopub.status.idle":"2024-12-04T15:46:05.690368Z","shell.execute_reply.started":"2024-12-04T15:45:57.695694Z","shell.execute_reply":"2024-12-04T15:46:05.689313Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Hyperparameters\nBATCH_SIZE = 32\nLEARNING_RATE = 2e-5\nEPOCHS = 5\nIMG_SIZE = 224  # Image size for DeiT\nDATASET_DIR = '/kaggle/input/lra-pathfinder-32/pathfinder32/curv_baseline'  # Update as needed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:46:05.691974Z","iopub.execute_input":"2024-12-04T15:46:05.692254Z","iopub.status.idle":"2024-12-04T15:46:05.696730Z","shell.execute_reply.started":"2024-12-04T15:46:05.692228Z","shell.execute_reply":"2024-12-04T15:46:05.695688Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Dataset Class\nclass PathfinderDataset(Dataset):\n    def __init__(self, data, transform=None):\n        self.data = data\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_path, label = self.data[idx]\n        try:\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            return image, label\n        except (IOError, OSError, Image.DecompressionBombError, Image.UnidentifiedImageError):\n            new_idx = (idx + 1) % len(self.data)\n            return self[new_idx]\n\n# Data Module\nclass PathfinderDataModule(pl.LightningDataModule):\n    def __init__(self, dataset_dir, batch_size=BATCH_SIZE):\n        super().__init__()\n        self.dataset_dir = dataset_dir\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n            transforms.ToTensor(),\n        ])\n\n    def prepare_data(self):\n        self.data_list = []\n        metadata_dir = os.path.join(self.dataset_dir, 'metadata')\n        for file_name in os.listdir(metadata_dir):\n            metadata_path = os.path.join(metadata_dir, file_name)\n            with open(metadata_path, 'r') as file:\n                for line in file:\n                    tokens = line.strip().split()\n                    img_rel_path = tokens[0] + \"/\" + tokens[1]\n                    label = int(tokens[3])\n                    img_path = os.path.join(self.dataset_dir, img_rel_path)\n                    self.data_list.append((img_path, label))\n\n    def setup(self, stage=None):\n        dataset = PathfinderDataset(self.data_list, transform=self.transform)\n        train_size = int(0.8 * len(dataset))\n        val_size = int(0.1 * len(dataset))\n        test_size = len(dataset) - train_size - val_size\n        self.train_set, self.val_set, self.test_set = random_split(dataset, [train_size, val_size, test_size])\n\n    def train_dataloader(self):\n        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=4)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=4)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_set, batch_size=self.batch_size, num_workers=4)\n\n# Initialize Data Module\ndata_module = PathfinderDataModule(DATASET_DIR)\ndata_module.prepare_data()\ndata_module.setup()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:46:05.697761Z","iopub.execute_input":"2024-12-04T15:46:05.698005Z","iopub.status.idle":"2024-12-04T15:46:08.350697Z","shell.execute_reply.started":"2024-12-04T15:46:05.697981Z","shell.execute_reply":"2024-12-04T15:46:08.349896Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Initialize DeiT model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = create_model('deit_small_patch16_224', pretrained=True, num_classes=2)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:47:01.381993Z","iopub.execute_input":"2024-12-04T15:47:01.382311Z","iopub.status.idle":"2024-12-04T15:47:02.859107Z","shell.execute_reply.started":"2024-12-04T15:47:01.382285Z","shell.execute_reply":"2024-12-04T15:47:02.858227Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/88.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efd2e8ed36284f808d234e51e2b3db33"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n    (norm): Identity()\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (patch_drop): Identity()\n  (norm_pre): Identity()\n  (blocks): Sequential(\n    (0): Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (1): Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (2): Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (3): Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (4): Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (5): Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (6): Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (7): Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (8): Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (9): Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (10): Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (11): Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n  (fc_norm): Identity()\n  (head_drop): Dropout(p=0.0, inplace=False)\n  (head): Linear(in_features=384, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def train_model(model, train_loader, optimizer, loss_fn, epochs=EPOCHS):\n    model.train()\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        total_loss = 0\n        for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = loss_fn(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_loader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:47:07.366081Z","iopub.execute_input":"2024-12-04T15:47:07.366858Z","iopub.status.idle":"2024-12-04T15:47:07.374381Z","shell.execute_reply.started":"2024-12-04T15:47:07.366809Z","shell.execute_reply":"2024-12-04T15:47:07.373353Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def evaluate_model(model, test_loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=-1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds)\n    weighted_metric = 0.7 * accuracy + 0.3 * f1  # Example weighted metric\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    print(f\"Test F1 Score: {f1:.4f}\")\n    print(f\"Weighted Metric: {weighted_metric:.4f}\")\n    return accuracy, f1, weighted_metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:47:12.302042Z","iopub.execute_input":"2024-12-04T15:47:12.302796Z","iopub.status.idle":"2024-12-04T15:47:12.309614Z","shell.execute_reply.started":"2024-12-04T15:47:12.302760Z","shell.execute_reply":"2024-12-04T15:47:12.308692Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Train and Evaluate\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\nloss_fn = torch.nn.CrossEntropyLoss()\n\ntrain_loader = data_module.train_dataloader()\ntest_loader = data_module.test_dataloader()\n\ntrain_model(model, train_loader, optimizer, loss_fn, epochs=EPOCHS)\naccuracy, f1, weighted_metric = evaluate_model(model, test_loader)\n\n# Save the model\ntorch.save(model.state_dict(), 'deit_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:47:16.885089Z","iopub.execute_input":"2024-12-04T15:47:16.885436Z","iopub.status.idle":"2024-12-04T18:28:07.646247Z","shell.execute_reply.started":"2024-12-04T15:47:16.885405Z","shell.execute_reply":"2024-12-04T18:28:07.645274Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1: 100%|██████████| 5000/5000 [31:39<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.6941\nEpoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|██████████| 5000/5000 [31:57<00:00,  2.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.5639\nEpoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3: 100%|██████████| 5000/5000 [31:57<00:00,  2.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.1941\nEpoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4: 100%|██████████| 5000/5000 [31:56<00:00,  2.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.1344\nEpoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5: 100%|██████████| 5000/5000 [31:57<00:00,  2.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.1062\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 625/625 [01:22<00:00,  7.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.9495\nTest F1 Score: 0.9495\nWeighted Metric: 0.9495\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}